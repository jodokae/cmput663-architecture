\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Impact of Architectural Change on
	Continuous Integration Build Outcome\\
}

\author{\IEEEauthorblockN{Johannes K{\"a}stle}
\IEEEauthorblockA{\textit{University of Alberta}\\
Edmonton, Canada \\
kaestle@ualberta.ca}
}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}

Continuous integration (CI) is nowadays common practice in software development \cite{CI-Common}. It helps finding faults earlier and therefore reducing the costs to fix them \cite{NutzenCI}. On the other hand, build failures decrease the pace of the development \cite{Costs-BuildFailures}. Hence, through decreasing the number broken builds, the speed of development is increased whereby costs are decreased.

There are numerous researches about predicting CI build failures. Within the context of the MSR 2017 challenge \cite{TravisTorrent},  different approaches emerged, using for example cascaded tree classifiers \cite{Pred-Cascade} or based upon the person who was submitting the commit \cite{ContrInvolv}. In general, tree classifiers are a useful machine learning (ML) technique to predict build outcome using meta data about this and previous commits \cite{Pred-Tree, ML-Project}. 

All ML approaches don't have high enough accuracy to be used alone. Because it has been shown that source code metrics impact the build outcome \cite{FailsCorr}, this paper wants to investigate this route further. 

Paix√£o et al. have found correlation between CI build outcomes and non-functional requirements \cite{Fail-NFReq}, and since these are often addressed by software architects \cite{NFR-Architects}, it is interesting if the software architecture is influencing the build results or itself is influenced by the continuous integration. 

To investigate the relationship between the CI and the architecture, it is necessary to measure the architectural change (AC) between two builds. Architectural change has been studied by different researchers in different contexts \cite{Aramis,StructDist,Arc-MDSE,Arcade-Base} , e.g.  finding the causes \cite{AC-Causes}, check consistency with the designed architecture \cite{ArcConf, ArcCons} investigating the impact on the business model \cite{ArcChange-Business} or the not-changed modules of the architecture \cite{Knowledge-AC}. 

The contribution of this work is the combination of architectural change with the prediction of continuous integration build outcomes. For that, the TravisTorrent dataset is exploited and the ten software systems with the highest numbers of commits are analyzed. Every build of those systems is reviewed for change to its predecessor using 8 different metrics. These are then compared against the build outcomes and used for trying to predict those. The author hypothesizes that change is followed by failing builds, because if there is a bigger problem in the system, see-able through failing builds, it is necessary to change the architecture. 

To investigate this problem an easy expandable framework is developed. It allows to run a tool chain which automatically downloads snapshots of a software system, reconstructs the architecture and computes change metrics. With only a few lines of code, new extractors, reconstructors and metric calculators can be added. Doing so, we reuse tools for all categories from previous researches, namely HUSACCT, ARCADE and Martin's metrics.

After calculating the change metrics for over $20,000$ commits using two different architecture reconstruction techniques, almost no correlation could be found between change and build outcome. In total, $48$ correlations are calculated with $32$ different configurations. The hypothesis that architectural change impacts the build outcome must therefore be rejected under the tested circumstances. 

\section{Related Works}

This section is split up into researches about continuous integration builds and architecture change and consistency. This work is a combination of these two parts and was not done in that combination before. 

\subsection{Continuous Integration}

Islam and Zebran \cite{FailsCorr} have studied the relationship between build outcome and project- as well as build metrics. They found out, that there is no correlation between size and the number of contributers in the project and build outcomes. Which tool is used for building the software (e.g. Maven or Ant) impacts the result of the build as well as the number of changed lines and files. Test code as well as the development branch did not impact the result. For our research it can be concluded that architecture, being affected by code and file changes, is a valid idea for investigation. 

In 2006 a 69\% specificity rate (detecting failures) was achieved using decision trees using meta data about the developer and the project itself \cite{Pred-Tree}.
The research was continued for the MSR '17 challenge by using cascading tree classifiers. They improved the prediction accuracy using information about previous builds, e.g. the outcome of the last build \cite{Pred-Cascade}. 
In that research, the prediction was only done internally in a project because is was shown that certain subsystems or developers are more erroneous. While this is certainly useful in practice, it does not help to mitigate the reasons of failing builds on a more general (i.e. cross-project) level.

\subsection{Architecture}

There are numerous approaches how architecture is researched in software engineering. Generally, there are two topics. First, the reconstruction of the implemented architecture in the code and, second, the interpretation of the architecture against the design or the architecture of different versions of the system.

The ARAMIS workbench \cite{Aramis} goes the step from static reconstruction of the architecture based on method calls and inheritance to extracting the data flow during runtime to check it against the predefined structure. 
For model-driven and generative software development, the task of extracting architecture is more complex than in traditional projects, because it passes through different layers of abstraction and possible multiple DSLs. 
Thus, the way of checking consistency must be account for different layers and is abstracted with architecture description languages (ADL) \cite{ArcCons,Arc-MDSE}. 
Also using ADLs, Haitzer et al. \cite{Arc-Decision} study architectural drift and erosion. In their system, the architect can simulate multiple implementation scenarios. With the help of the ADL, the programm then calculates the consequences of this change, and the architect can make an educated decision which change has the best impact on the system and its architecture. 

Caracciolo et al. \cite{ArcConf} found that the automated checking of the implemented architecture with its set one, leads to fewer violation of the architecture. They extracted the architecture using classic static analysis and a dependency graph. Their system checks then for violations of the MVC pattern and reports it directly to the developer. Over the course of time fewer violations were found compared to a control group, which shows that the developers learned how to avoid architecture violations.

Nakamura and Basili \cite{StructDist} introduced the measuring of architectural distance using kernels. For that, both architectures need to be represented as a graph structure (here OO class structure) and are then compared for similar substructures. This negates the problems of renaming and is applicable to every graph. 
Because it works on the complete class graph and not an abstracted version, i.e. the architecture, it is not viable for consistency checking, as the designed architecture is not represented down to the class level. 
Since the graph is compared to kernels, it is necessary to define a distance measurement for this. A similar approach is used by Garcia \cite{arcade-thesis} with the cluster coverage metric.

Tonu et al. \cite{Swag} have research architecture stability, primarily in C and C++ projects, but also introduced support for Java.\footnote{\url{http://www.swag.uwaterloo.ca/javex/}} Using four different types of metrics, growth, change, cohesion and coupling they try to analyze when the architecture of a software system stabilizes and then predict stability for future versions. It does so by extracting facts from the source code (or in case of Java byte code) and reconstructing the architecture. A fact can be anything, from LoC to function calls and classes. 
In both studied projects, the architecture stabilizes relatively quickly and has only small changes afterwards. Unfortunately, we can not use this tool in our research, because we want to study the source files and not the byte code of Java projects. But we will analyze change and coupling metrics.

HUSAACT \cite{Husacct1,Husacct2} is a highly customizable architecture conformance framework for Java and C\#. Dependencies between architecture modules can be defined in multiple ways and different kinds of dependencies are possible. They argument, that a ``call'' dependency cannot be compared directly to a ``inherits'' dependency and, hence, must be treated differently. 
It extracts several types of dependencies between source code elements (e.g. classes, interfaces, packages). HUSAACT needs to have a module specification, i.e. which root packages or classes are part of which architecture module. If given this, it maps the defined architecture vs the real implementation and reports violations.
If no such specification is available it is not possible to compare two versions of software on a higher level than the implementation graph. 

In his PhD thesis \cite{arcade-thesis}, Garcia developed the ARCADE framework. It is used to detect architectural decay, also called thrift and erosion, as well as architecture smells. During this, various recovery techniques were compared \cite{arcRec-comparison}, whereat ACDC \cite{ACDC} and ARC were the most successful. 
ACDC clusters modules based on patterns, for example being in the same package, are used together or elements that both depend on the same resources. For Java, ACDC recovers the architecture from the class files. ARC is introduced by Garcia himself, clusters entities based on semantic similarity using a statistical language model based on comments and identifiers in the source code.
The ARCADE framework uses these different recovery techniques to compute similarity metrics, namely a2a (architecture to architecture) and cvg (cluster coverage) \cite{Arcade}. The first one defines similarity based on the minimum number of steps to get from one architecture graph to the other. For example, removing one dependency or adding one node is each one action. While a2a is looking on the architecture from a top level perspective, cvg looks into the modules, or clusters, and computes if they are still the same module. For example, if all classes in a module are changed but the module name itself is untouched, it can't still be considered to be the same module.

As one reason for failing builds are syntax errors, and therefore uncompilable, we do not want to use recovery techniques which rely on the compiled class files. Therefore, we do not use the SWAG Kit or ARCADE directly.
But the metrics and the recovery technique sound promising, thus this research wants to reuse them. Therefore, we combine the strengths of HUSAACT in extracting the structure from the source files with the recovery technique ACDC and the metrics from ARCADE. The cvg metric is comparable to kernel based similarity whereas a2a considers change. Coupling in the architecture will be addressed with Martin's metrics.

\section{Background}

The TravisTorrent dataset \cite{TravisTorrent} consists of over 3.5 millions samples of builds done in TravisCI from over 1,300 projects, written in Java, JavaScript or Ruby. TravisCI is a continuous integration tool which is tightly integrated into GitHub. It is used to automatically compile, build and test a software system, whenever new code is commited to the repository. This helps the developer to find bugs and should increase the development speed. 
TravisTorrent saves over fifty features for every build, including the project, the built commit id and the result.

Software architecture is usually a graph of the high-level components, called modules, of a software system which are connected through links, called dependencies. This architecture can be modeled with different architecture description languages (ADL), like the UML, though there is standardized way \cite{UML-Arch}. 
As there is no single way for describing, there is no best way for reverse engineering the architecture. The most basic way for that is using the package structure. This assumes that the developers have thought about the architecture and translated it into packages. The high level packages are then the modules, and are dependent if one of their implementing elements is using an element of another one. 
Other approaches for reconstructing the architecture ACDC or ARC as introduced in the previous section.

To measure then the similarity or change between two architectures, of even between the designed and the implemented architecture, there are several approaches. For OO software systems, as studied here, the so called Martin's metrics \cite{martinsMetrics} were introduced, i.e. afferent, efferent coupling and instability. Those metrics describe the independence of modules. Coupling describes how many incoming or outgoing transitions a module has and instability is the ratio between efferent coupling and the complete coupling. 
In the context of architectural change that can be used in the following way: If the instability changes, if considering that between two modules can only be one link, that this module has an increase or decrease in the number of dependencies. This is not the case, if the module has the same change in afferent as well as in efferent coupling, but this is unlikely and as every change metric, this is also only a heuristic. 

More complex change metrics are a2a and cvg. Architecture to architecture measures the minimum amount of steps to get from the first to the second architecture divided by the sum of the steps needed to build both architectures them self. A step is either the adding, removing or moving of an entity inside a module or the creation or deletion of a module. 
The cluster coverage (cvg) is the metric that measures how the inside of modules are changing. For that first the cluster to cluster (c2c) metric is calculated, which defines how many elements of the cluster overlap divided by the number of elements in the bigger cluster. If this ratio is above a predefined threshold, the clusters are called equal. The cvg metric then calculates the ratio between equal modules and the amount of modules in the architecture. In ARCARDE the cvg is calculated with a threshold of $75\%$ from the earlier version to the latter one and with $66\%$ the other way around. 

The problem with a2a and cvg is, that they don't explicitly take the dependencies between the modules into account. This is only done while calculating the architecture itself, which is done via ACDC. This is the reason why those two metrics are combined with the widely accepted Martin's metrics which only consider the dependencies.

\section{Methodology}

From the TravisTorrent dataset, the ten largest Java projects, in terms of number of commits, were analyzed in this study. Doing so ensures that there is enough data for making statistical reasonable results over a course of time for a project. A list of analyzed projects can be found in table \ref{tableProjects}.

Because the built tool impacts the outcome of the build but not on the architecture itself, we will limit the study to only one build tool. This introduces a little bit of bias but ensures consistent results. Because it is necessary to know where the source files reside inside the project, we choose the build tool maven to get the advantage of its convention over configuration paradigm. In this way we can be sure where the source files and where resources and test lie in the file structure. This eases the implementation of the class structure extraction. 
Because there are possibly multiple builds for the same commit, we only use the build which builds a certain version of the software first, because this ensures that the change is directly correlated to the first build that introduces this change. These restrictions lower the number of considered builds by $x\%$. 
%TODO Numbers and Table

In general, to get an architectural change, we have to follow three steps. First, we need to extract the class structure out of the source code, then we reconstruct the architecture and eventually we take two architectures and compare them with each other to get a change metric (fig \ref{overview}). The complete source code and results can be found in the author's GitHub repository.\footnote{\url{https://github.com/jodokae/cmput663-architecture}}

\begin{figure*}
	\centering
	\includegraphics[width=7in]{assets/overview.pdf}
	\caption{Overview of the process}
	\label{overview}
\end{figure*}


\subsection{Framework}

\begin{figure}
	\centering
	\includegraphics[width=3.45in]{assets/architecture.pdf}
	\caption{Structure of the Framework}
	\label{frameworkStructure}
\end{figure}

The extraction of the architectural change metrics between two commits is implemented using a framework. The general structure (fig. \ref{frameworkStructure}) extracts the unique commit ID for GitHub out of the database, then downloads the commit and extracts the architecture out of the source files. The architecture is saved in a file using the Rigi Standard Format (rsf) \cite{RSF}, because the ARCADE tools rely on this filetype. This filetype saves simple relation triples in the form: type, source, target. Then, for every two subsequent commits the architectures are compared and the results are saved in a JSON file. The advantage of this framework model is, that all modules can simply be exchanged, and a new database or metric calculator can be implemented with little effort.

In the following the implemented tools for the chain are explained. An overview can be found in figure \ref{implToolchain}.

\begin{figure}
	\centering
	\includegraphics[width=3.45in]{assets/implementedArc.pdf}
	\caption{Implemented tool chain}
	\label{implToolchain}
\end{figure}


\subsubsection{Reconstruction}

For the reconstruction two different approaches are implemented, the first one being ACDC, which was evaluated to have the best reconstruction accuracy \cite{arcRec-comparison} in comparison with other common techniques. For the ground truth analysis the package structure is also computed. 

ACDC, in its implementation given in the ARCADE framework, is using the class files for computation. Given that a prominent reason for failing builds is compilation error this is problematic. Therefore, the extractor of the HUSACCT framework is used for extracting the class files. For simplicity, and because it is not used in the further analyses, the dependency types are restricted to ``subPkg'', ``contains'' and ``references'', where ``contains'' means that that a package contains a Java type and ``references'' is any dependency between two Java types. The results are then fed into the reconstruction. The package architecture is based on the HUSACCT results, as well. 

The package reconstructor looks for the root packages in the system. If there are only a few root packages, it is assumed that the implemented architecture lies a level deeper in the packages. For example, if there is only the ``org.sonar'' as root, this is just the base container and not an architectural element. Therefore, all direct children of this package are considered as modules until there are at least an acceptable number of modules. In this work, 10 was considered the minimum amount of modules needed. As dependencies all ``references'' edges were considered and added to their respective parent module. This is a valid approach according to Song et al \cite{ArcAsGraph}. 

\subsubsection{Change Metrics}

As explained in the last paragraph of the background Section, the ARCADE metrics a2a and cvg as well as Martin's metrics are used. ARCADE's metrics implementation rely on the knowledge how the architecture was extracted. Therefore, it is not possible to calculate a2a and cvg on the package based architecture graph without changing the ARCADE source code. In order to still use the package based architecture, Martin's metrics are calculated on this graph. For every module the absolute number of incoming and outgoing edges are calculated, the degree of the node (sum over ingoing and outgoing edges), as well as the absolute and relative instability. The absolute instability is based on the absolute number of connections between the modules, i.e. if Module A has only three ingoing connections from Module B, then the afferent coupling is considered three. For the relative instability, the coupling is instead considered one, so it is not measured how tightly connected two modules are, just if they are connected at all.

To get some global numbers for the complete architecture, the mean instabilities and node degree are calculated as well as the number of nodes and edges in the graph.

The metrics are then compared pairwise. To compare proportional metrics, like instability, the difference between the values is taken, e. g. if architecture A has an average instability of $30\%$, and architecture B has an average instability of $25\%$, then they are $5\%$ different to each other. For metrics with absolute numbers, like node degree, the similarity is the proportion of the two values, e.g. A has an average node degree of $8$ and B has $10$, then they are $80\%$ similar. 

\subsection{Evaluation}

The calculated metrics are tested for correlation with the build result. This is done in multiple ways. It is tested against the direct build outcome, then the previous or following $b$ outcomes for $b \in \{2, 3, 5, 10\}$. Then, the metric is converted to a boolean change or no change with a given threshold $t$, where everything below $t$ is considered as no change, and anything above as a change for $t \in \{0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5\}$. These values were chosen because most found changes were small. Every comparison with includes no boolean variables is run with Pearson correlation test, the others with Spearman's test. 

\section{Results}

This section presents the findings of the proposed methodology. First, the projects under study are introduced, then calculated metrics are evaluated and, eventually, the correlation between the metrics and the build outcomes is shown.

\subsection{Studied Projects}

\input{assets/projects}

For this study then ten biggest projects, according to the number of commits, out of the TravisTorrent projects are analyzed. Only projects and builds which are written in Java and use Maven as a build tool are considered. Because of change of configuration it is possible that some versions of the software is built multiple times. Since it is impossible that those builds have architectural change, only the first build per commit is considered. In the end, almost $21,000$ builds were studied with a pass rate of $83\%$. The builds range between one and four years of development time, some start at a well established state (Sonarqube at build number over 500), others at build 1 (e.g. Checkstyle). The projects vary in their application field, there are two code analyzers, three APIs, a webclient and an IDE, some for databases, linear algebra or ontologies. Table \ref{tableProjects} gives a detailed list.

\subsection{Metric Evaluation}

\input{assets/metricCorr}
\input{assets/metrics}

In table \ref{tableMetricCorr} the correlation between the metrics is shown. All p values were or close to 0. Most metrics are highly correlated and all at least somehow correlated. This is good, because it gives indication that all metrics are truly an indicator for architectural change if we assume that at least one of them is. This strengthens the confidence in the chosen tool chain. 

Depending on the metric, most builds show no change (table \ref{tableMetric}). Only for a2a there are more builds with a change than without one. But then most of these changes are really small, as shown in figure \ref{a2aHist}. Most changes are quite small in other metrics as well, with only few big architectural changes (figure \ref{cvgHist} and \ref{numEdgesHist}). This is to be expected that architecture changes only sometimes in a big manner.

\begin{figure}
	\centering
	\includegraphics[width=3in]{assets/a2a.pdf}
	\caption{Histogram of the a2a metric over all projects}
	\label{a2aHist}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=3in]{assets/cvgSource.pdf}
	\caption{Histogram of the cvg src metric over all projects}
	\label{cvgHist}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=3in]{assets/NumEdges.pdf}
	\caption{Histogram of the \#E metric over all projects}
	\label{numEdgesHist}
\end{figure}

\subsection{Correlation}

\begin{figure}
\centering
\includegraphics[width=3in]{assets/cvgSourceCorrPlot}
\caption{cvg src vs next five builds over different change thresholds }
\label{cvgPlot}
\end{figure}


Most found correlations, with a p-value under $5\%$, were between $0.1$ and $3\%$. Only some metrics show a higher correlation around $9-11\%$. The threshold how many previous or following builds were considered, had no significant impact. The highest correlation found was for cvg, regardless of source or target, with a change threshold of $0.0$, i.e. every little change was considered as change (figure \ref{cvgPlot}). But even the best correlation was only $11\%$.

\section{Discussion}

While checking ten big Java projects, no significant correlation between architectural change and build results was found. Apart from there actually being no correlation, there could be various factors influencing this result. First of all, only ten projects, although from different domains, were evaluated. Despite having projects which start at build one, the number of failing builds was relatively small. It is possible that the missing connection between architecture and build results lies there that all considered projects have a well thought-through architecture and do change it in a way that in produces problems. 

It is also possible, that the problems which are produced by architectural change are not transfered into build results but to a different level, e.g.(1) code coverage or (2) number failed tests. Unfortunately the dataset does not give the opportunity to check for those, because they are (1) not reported and (b) have missing values in around $20\%$ of the builds. That software architecture is important is common knowledge. That it changes over the course of a project was shown. But it remains unknown how change affects the progress of the development process. 

As \cite{arcRec-comparison} shows even the best recovery techniques have problems detecting the true architecture. Therefore, it is possible that HUSACCT and ARCADE are not the right tools for the research question. This problem was adressed since we consider multiple recovery techniques. The produces metrics highly correlate, which strengthens the claim that the change detection is working as intended. 

In the future, different metrics and extractors should be added to the comparison as well as more projects should be considered. This will improve the trust in our findings, that architectural change does not impact the build results significantly. This is easily possible due to the nature of the framework.

\section{Conclusion}

This research conducts a large scale architecture recovery and change analysis for over $20,000$ builds from $10$ different Java Maven projects from various domains. For that a easily expandable framework was developed. Then, $8$ metrics using $2$ different recovery techniques were extracted. The hypothesis that architectural change has an impact on CI build outcomes at or near the change was rejected. The highest correlation found was around $11\%$. This could be due to the well established nature of the studied projects, or that the chosen metrics and techniques are not suitable for the problem. The built framework can be efficiently extended to consider more recovery tools and metrics. The hypothesis of the author is that the impact from architectural change has to be lie somewhere else but in the outcome of the build.

Check References Einheitlich

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,literature}

\end{document}
\grid
