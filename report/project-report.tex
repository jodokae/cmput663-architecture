\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Impact of Architectural Change on
	Continuous Integration Build Outcome\\
}

\author{\IEEEauthorblockN{Johannes K{\"a}stle}
\IEEEauthorblockA{\textit{University of Alberta}\\
Edmonton, Canada \\
kaestle@ualberta.ca}
}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\begin{IEEEkeywords}
???
\end{IEEEkeywords}

\section{Introduction}

Continuous integration (CI) is nowadays common practice in software development \cite{CI-Common}. It helps finding faults in the system earlier and therefore reducing the costs to fix them \cite{NutzenCI}. On the other hand, build failures decrease the pace of the development \cite{Costs-BuildFailures}. Hence, through decreasing the number broken builds, the speed of development is increased whereby the its costs are decreased.

The are numerous researches about predicting CI build failures. Within the context of the MSR 2017 challenge \cite{TravisTorrent},  different approaches emerged, using for example cascaded tree classifiers \cite{Pred-Cascade} or based upon the person who was submitting the commit \cite{ContrInvolv}. In general, tree classifiers are a useful machine learning (ML) technique to predict build outcome using meta data about this and previous commits \cite{Pred-Tree, ML-Project}. 

All ML approaches don't have high enough accuracy to be used alone. Because it has been shown, that source code metrics impact the build outcome \cite{FailsCorr}, this paper wants to investigate this route further. 

Paix√£o et al. have found correlation between CI build outcomes and non-functional requirements \cite{Fail-NFReq}, and since these are often addressed by software architects \cite{NFR-Architects}, it is interesting if the software architecture is influencing the build results or itself is influenced by the continuous integration. 

To investigate the relationship between the CI and the architecture, it is necessary to measure the architectural change (AC) between two builds. Architectural change has been studied by different researchers in different contexts \cite{Aramis,StructDist,Arc-MDSE,Arcade-Base} , e.g.  finding the causes \cite{AC-Causes}, check consistency with the designed architecture \cite{ArcConf, ArcCons} investigating the impact on the business model \cite{ArcChange-Business} or the not-changed modules of the architecture \cite{Knowledge-AC}. 

The contribution of this work is the combination of architectural change with the prediction of continuous integration build outcomes. For that, the TravisTorrent dataset is exploited and the ten software systems with the highest numbers of commits are analyzed. Every build of those systems is reviewed for change to its predecessor using different metrics. These are then compared against the build outcomes and used for trying to predict those.

%TODO Results in Introduction
TODO: Results Short

\section{Related Works}

This section is split up into researches about continuous integration builds and architecture change and consistency. This work is a combination of this two parts and unprecedented. 

\subsection{Continuous Integration}

Islam and Zebran \cite{FailsCorr} have studied the relationship between different metrics of the project and builds and the outcome. They found out, that there is no correlation between size and the number of contributers in the project and build outcomes. The used build tools impacts the result of the build as well as the number of changed lines and files. Test code as well as the branch did not impact the result. For this work it can be concluded that architecture, being affected by code and file changes, is a valid idea for investigation. 

In 2006 a 69\% specificity rate (detecting failures) was achieved using decision trees using meta data about the developer and the project itself \cite{Pred-Tree}.
The research was continued for the MSR '17 challenge by using cascading tree classifiers. They improved the prediction accuracy using information about previous builds, e.g. the result of the last build \cite{Pred-Cascade}. 
Here the focus lay on predicting the build for a certain project because is was shown that certain subsystems or developers are more erroneous. While it is certainly useful in practice, it does not help to mitigate the reasons of failing builds on a global (not project-wise) level.

\subsection{Architecture}

% TODO Swag Kit Waterloo

There are numerous different approaches how architecture is researched in software engineering. Generally, there are two kinds of topics. First, the reconstruction of the implemented architecture in the code and, second, the interpretation of the architecture against the predefined one or the architecture of different versions of the system.

The ARAMIS workbench \cite{Aramis} goes the step from the static reconstruction of the architecture based on method calls and inheritance to extracting the dataflow during runtime of the program to check it against the predefined structure. 
For model-driven and generative software development, the task of extracting the architecture is more complex than in traditional projects, because it goes through different layers of abstraction and possible multiple DSLs and changes the way to check its consistency through architecture description languages (ADL) \cite{ArcCons,Arc-MDSE}. Also using ADLs, Haitzer et al. \cite{Arc-Decision} study architectural drift and erosion. For that, the architect can play through multiple implementation scenarios to see the impact of this choice on the architecture.

Caracciolo et al \cite{ArcConf} found that the automated checking of the implemented architecture with its set one, leads to fewer violation of the architecture. They extracted the architecture using classic static analysis and a dependency graph.  

Nakamura and Basili \cite{StructDist} introduced the measuring of architectural distance using kernels. For that, both architectures need to be represented as a graph structure (here OO class structure) and are then compared for similar substructures. This negates the problems of renaming and is applicable to every graph. Problematic is, that it does not work on the architecture itself but on the complete implementation graph, which makes it not usable for consistency checking. Since the graph is compared to kernels, it is necessary to define a distance measurement for this.

HUSAACT \cite{Husacct1,Husacct2} is a highly customizable architecture conformance framework for Java and C\#. Dependencies between architecture modules can be defined in multiple ways and different kinds of dependencies are possible. They argument, that a 'call' dependency cannot be compared directly to a 'inherits' dependency and, hence, must be treated differently. 
It extracts several types of dependencies between source code elements (e.g. classes, interfaces, packages). HUSAACT needs to have a module specification, i.e. which root packages or classes are part of which architecture module. If given this, it maps the defined architecture vs the real implementation and reports violations.
If no such specification is available it is not possible to compare two versions of software on a higher level than the implementation graph. 

In his PhD thesis \cite{arcade-thesis}, Garcia developed the ARCADE framework. It is used to detect architectural decay, also called thrift and erosion, as well as architecture smells. During this, different recovery techniques were compared \cite{arcRec-comparison}, whereat ACDC \cite{ACDC} and ARC were the most successful. 
ACDC clusters modules based on patterns, for example being in the same package, are used together or elements that both depend on the same resources. For Java, ACDC recovers the architecture from the class files. ARC is introduced by Garcia himself, clusters entities based on semantic similarity using a statistical language model based on comments and identifiers in the source code.
The ARCADE framework can then use these different recovery techniques to compute similarity metrics, namely a2a (architecture to architecture) and cvg (cluster coverage) \cite{Arcade}. The first one defines similarity based on the number of steps that are necessary to get from one architecture graph to the other. For example, removing one dependency or adding one node is each one action. While a2a is looking on the architecture from the outside, cvg looks into the modules, or clusters, and computes if they are still the same module. For example, if one changes all classes in a module, but leaves the module name itself, it can't still be considered to be the same module.

Because one reason for failing builds is the inability to compile the source code, it is undesirable to use a technique which relies on the compiled class files. But the metrics and the recovery technique sound promising, therefore this research wants to reuse them. Therefore, it combines the strengths of HUSAACT in extracting the structure from the source files with the recovery technique ACDC and the metrics from ARCADE.

\section{Background}

The TravisTorrent dataset \cite{TravisTorrent} consists of over 3.5 millions samples of builds done in TravisCI from over 1,300 projects, written in Java, JavaScript or Ruby. TravisCI is a continuous integration tool which is tightly integrated into GitHub. It is used to automatically compile, build and test a software system, whenever new code is commited to the repository. This helps the developer to find bugs and should increase the development speed. 
TravisTorrent saves over fifty features for every build, including the project, the built commit id and the result.

Software architecture is usually a graph of the high-level components, called modules, of a software system which are connected through links, called dependencies. This architecture can be modeled with different architecture description languages (ADL), like the UML, though there is standardized way \cite{UML-Arch}. 
As there is no single way for describing, there is no best way for reverse engineering the architecture. The most basic way for that is using the package structure. This assumes that the developers have thought about the architecture and translated it into packages. The high level packages are then the modules, and are dependent if one of their implementing elements is using an element of another one. 
Other approaches for reconstructing the architecture ACDC or ARC as introduced in the previous section.

To measure then the similarity or change between two architectures, of even between the designed and the implemented architecture, there are several approaches. For OO software systems, as studied here, the so called Martin's metrics \cite{martinsMetrics} were introduced, i.e. afferent, efferent coupling and instability. Those metrics describe the independence of modules. Coupling describes how many incoming or outgoing transitions a module has and instability is the ratio between efferent coupling and the complete coupling. 
In the context of architectural change that can be used in the following way: If the instability changes, if considering that between two modules can only be one link, that this module has an increase or decrease in the number of dependencies. This is not the case, if the module has the same change in afferent as well as in efferent coupling, but this is unlikely and as every change metric, this is also only a heuristic. 

More complex change metrics are a2a and cvg. Architecture to architecture measures the minimum amount of steps to get from the first to the second architecture divided by the sum of the steps needed to build both architectures them self. A step is either the adding, removing or moving of an entity inside a module or the creation or deletion of a module. 
The cluster coverage (cvg) is the metric that measures how the inside of modules are changing. For that first the cluster to cluster (c2c) metric is calculated, which defines how many elements of the cluster overlap divided by the number of elements in the bigger cluster. If this ratio is above a predefined threshold, the clusters are called equal. The cvg metric then calculates the ratio between equal modules and the amount of modules in the architecture. In ARCARDE the cvg is calculated with a threshold of $75\%$ from the earlier version to the latter one and with $66\%$ the other way around. 

The problem with a2a and cvg is, that they don't explicitly take the dependencies between the modules into account. This is only done while calculating the architecture itself, which is done via ACDC. This is the reason why those two metrics are combined with the widely accepted Martin's metrics which only consider the dependencies.

\section{Methodology}

From the TravisTorrent dataset, the ten largest Java projects, in terms of number of commits, were analyzed in this study. Doing so ensures that there is enough data for making statistical reasonable results over a course of time for a project. A list of analyzed projects can be found in table \ref{tableProjects}.

Because the built tool impacts the outcome of the build but not on the architecture itself, we will limit the study to only one build tool. This introduces a little bit of bias but ensures consistent results. Because it is necessary to know where the source files reside inside the project, we choose the build tool maven to get the advantage of its convention over configuration paradigm. In this way we can be sure where the source files and where resources and test lie in the file structure. This eases the implementation of the class structure extraction. 
Because there are possibly multiple builds for the same commit, we only use the build which builds a certain version of the software first, because this ensures that the change is directly correlated to the first build that introduces this change. These restrictions lower the number of considered builds by $x\%$. 
%TODO Numbers and Table

\subsection{Framework}

The extraction of the architectural change metrics between two commits is implemented using a framework. The general structure (fig. \ref{frameworkStructure}) extracts the unique commit ID for GitHub out of the database, then downloads the commit and extracts the architecture out of the source files. The architecture is saved in a file using the Rigi Standard Format (rsf) \cite{RSF}, because the ARCADE tools rely on this filetype. This filetype saves simple relation triples in the form: type, source, target. Then, for every two subsequent commits the architectures are compared and the results are saved in a JSON file. The advantage of this framework model is, that all modules can simply be exchanged, and a new database or metric calculator can be implemented with little effort.

In the following the implemented tools for the chain are explained. An overview can be found in figure \ref{implToolchain}.

\subsubsection{Reconstruction}

For the reconstruction two different approaches are implemented, the first one being ACDC, which was evaluated to have the best reconstruction accuracy \cite{arcRec-comparison} in comparison with other common techniques. For the ground truth analysis the package structure is also computed. 

ACDC, in its implementation given in the ARCADE framework, is using the class files for computation. Given that a prominent reason for failing builds is compilation error this is problematic. Therefore, the extractor of the HUSACCT framework is used for extracting the class files. For simplicity, and because it is not used in the further analyses, the dependency types are restricted to 'subPkg', 'contains' and 'references', where 'contains' means that that a package contains a Java type and 'references' is any dependency between two Java types. The results are then fed into the reconstruction. The package architecture is based on the HUSACCT results, as well. 

The package reconstructor looks for the root packages in the system. If there are only a few root packages, it is assumed that the implemented architecture lies a level deeper in the packages. For example, if there is only the 'org.sonar' as root, this is just the base container and not an architectural element. Therefore, all direct children of this package are considered as modules until there are at least an acceptable number of modules. In this work, 10 was considered the minimum amount of modules needed. As dependencies all 'references' edges were considered and added to their respective parent module. This is a valid approach according to Song et al \cite{ArcAsGraph}. 

\subsubsection{Change Metrics}

As explained in the last paragraph of the background Section, the ARCADE metrics a2a and cvg as well as Martin's metrics are used. ARCADE's metrics implementation rely on the knowledge how the architecture was extracted. Therefore, it is not possible to calculate a2a and cvg on the package based architecture graph without changing the ARCADE source code. In order to still use the package based architecture, Martin's metrics are calculated on this graph. For every module the absolute number of incoming and outgoing edges are calculated, the degree of the node (sum over ingoing and outgoing edges), as well as the absolute and relative instability. The absolute instability is based on the absolute number of connections between the modules, i.e. if Module A has only three ingoing connections from Module B, then the afferent coupling is considered three. For the relative instability, the coupling is instead considered one, so it is not measured how tightly connected two modules are, just if they are connected at all.

To get some global numbers for the complete architecture, the mean instabilities and node degree are calculated as well as the number of nodes and edges in the graph.

The metrics are then compared pairwise. To compare proportional metrics, like instability, the difference between the values is taken, e. g. if architecture A has an average instability of $30\%$, and architecture B has an average instability of $25\%$, then they are $5\%$ different to each other. For metrics with absolute numbers, like node degree, the similarity is the proportion of the two values, e.g. A has an average node degree of $8$ and B has $10$, then they are $80\%$ similar. 

\subsection{Evaluation}

%\section{Implementation}

%Needed or in Framework?

\section{Results}

\section{Threats to Validity}

\section{Conclusion}

\section*{Acknowledgment}

Needed?
Check References Einheitlich

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,literature}

\end{document}
