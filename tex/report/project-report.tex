\documentclass[sigplan, anonymous, review]{acmart}
%\documentclass[sigconf]{acmart}

%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{array}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{numprint}
%\usepackage{booktabs} % For formal tables
%\usepackage{balance}

\newcommand{\sn}[1]{{\color{blue}\textbf{Sarah:}~#1}}
\newcommand{\jk}[1]{{\color{violet}\textbf{Johannes:}~#1}}
\newcommand{\checkNum}[1]{{\color{orange}#1}}
\newcommand{\todo}[1]{{ \color{red} \textbf{TODO:}~#1}}

%\keywords{ACM proceedings, \LaTeX, text tagging}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column

\begin{document}

\title[Architectural Change vs CI Build Outcome]{On the Relationship Between Architectural Changes and Continuous Integration Build Outcome
}

\author{Johannes K{\"a}stle}
\affiliation{%
	\institution{University of Alberta}
	\city{Edmonton}
	\country{Canada}
}
\email{kaestle@ualberta.ca}

\author{Sarah Nadi}
\affiliation{%
	\institution{University of Alberta}
	\city{Edmonton}
	\country{Canada}
}
\email{nadi@ualberta.ca}

\begin{abstract}
Continuous Integration (CI) is becoming an essential component in the software development process.
However, developers often need to spend time fixing broken CI builds.
Therefore, identifying reasons for build failure and warning developers about these likely failures can save valuable time and cost for developers.
In this paper, we investigate if there is a relationship between architectural changes and CI build outcomes.
We form two hypotheses about the relationship: (1) architectural changes lead to higher CI build failures and (2) higher CI build failures lead to architectural changes.
To investigate these hypotheses, we design a fully automated extensible framework that analyzes the architectural changes between two consecutive versions of a software system.
We use three well-established techniques to reconstruct the projects' architecture from the source code and measure change with eight metrics. 
In addition, we analyze the build logs to investigate the point of failure during the build. 
We mine almost 50,000 builds from 159 open-source Java repositories, but find no significant correlation between architectural change and build outcome. 
To enable other researchers to replicate our negative results and to investigate additional architectural metrics, we publish our framework which can easily be extended for new research questions and metrics.
\end{abstract}

\maketitle

\section{Introduction}

Continuous integration (CI) is nowadays a common practice in software development \cite{CI-Common}. It helps finding faults earlier and therefore reduces the costs to fix them \cite{NutzenCI}. On the other hand, build failures decrease the development pace \cite{Costs-BuildFailures}. Hence, by decreasing the number broken builds, the speed of development is increased whereby costs are decreased.

There are numerous research directions for predicting CI build failures. Within the context of the MSR 2017 challenge \cite{TravisTorrent}, different approaches emerged.
Multiple researches have used meta-data about the person who submitted the triggering commit \cite{ContrInvolv} or meta-data from previous commits \cite{Pred-Cascade}. The latter study showed that cascaded tree classifiers achieve the best results with this type of data. 
In general, tree classifiers have been shown to be a useful machine learning (ML) technique to predict build outcome using meta data about the current and previous commits \cite{Pred-Tree}. 
However, meta-data based ML approaches alone do not have high accuracy.
Previous work has shown that in addition to meta-data about a commit and its contributors, source code metrics, such as changed lines of source or test code, also impact the build outcome \cite{FailsCorr}. The impact of source code metrics on bugs and failures has been investigated by various studies \cite{MetricsSource1, MetricsSource2}. 

Bengtsson \cite{arc-metrics} showed that existing source code metrics can be used to measure software architecture. 
Given such a relationship, and the fact that source code metrics affect the build outcome~\cite{MetricsSource1, MetricsSource2, FailsCorr}, then changes in software architecture may also have an impact on the build outcome.
Furthermore, Paix\~{a}o et al. found a correlation between CI build outcomes and restrictions in the form of non-functional requirements \cite{Fail-NFReq}, e.g., restrictions on response time.
Since non-functional requirements are often addressed by software architects \cite{NFR-Architects}, it is interesting to investigate if the software architecture influences the build results or is itself influenced by continuous integration. 
This could happen if, for example, multiple builds fail and, thus, the architecture gets changed to improve the problematic area in the source code.

\sn{don't think we need this next part in the intro.. if this is all work discussed in the intro, I would remove it here: To investigate the relationship between CI and software architecture, it is necessary to measure the architectural change (AC) between two builds. Architectural change has been studied by different researchers in different contexts \cite{Aramis,StructDist,Arc-MDSE,Arcade-Base}, including  finding the causes for change \cite{AC-Causes}, checking consistency with the designed architecture \cite{ArcConf, ArcCons}, and investigating the impact of the change on the business model \cite{ArcChange-Business} or the non-changed modules of the architecture \cite{Knowledge-AC}. }
\jk{do you want to remove the next part completely or just move it maybe to Rel Work? I like the implication done by that work, so I would like to to keep it somewhere. We're talking about the paragraph based on \cite{ImpactAwareness}, right?}

Paixao et al. \cite{ImpactAwareness} show that most developers are not aware that their code changes actually affect the architecture of the system. When made aware of the relationship, developers tend to be more careful abut their changes and they try to improve the overall architecture of the system.
Based on such previous findings, we argue that if developers realize that any architecture changes they make influence the build outcome in CI, something that affects them on a daily basis, this would further increase their awareness about the importance of the architecture of the system.

In this work, we explore the relationship between architectural change and CI build outcome.
We analyze $159$ Java Maven software systems from the TravisTorrent dataset~\cite{TravisTorrent}. 
We extract the architectural changes in every CI build by comparing the corresponding version of the system to its predecessor.
We use eight different architecture metrics from the literature to evaluate architectural changes.  
We store those extracted architectural changes along with the outcome of the build and use this data to investigate the relation between architecture and build outcome.
We hypothesize that architectural changes are either (1) preceded or (2) followed by failing builds. 
Our intuition for the first hypothesis is that many failing builds indicate a big problem in the system, which might entail changing the architecture to solve it. 
On the other hand, our intuition for the second hypothesis is that a change in the architecture may result in unexpected consequences that would then lead to build failures.

We propose the following null hypothesis:

\paragraph{$\mathbf{H_0}$:} Architectural change does not correlate with Continuous Integration build outcome. 
\vspace{0.2cm}

To automate our investigation, we develop an extensible framework that runs a toolchain to automatically download snapshots of a software system, reconstruct the architecture, and compute change metrics. With only a few lines of code, new extractors, reconstructors, and metric calculators can be added. 
The architecture of our framework allows us to easily reuse existing architectural extraction tools and metrics, namely HUSACCT \cite{Husacct1}, ARCADE \cite{Arcade}, and Martin's metrics \cite{martinsMetrics}.

Our analysis of almost $50,000$ commits using two different architecture reconstruction techniques shows that almost no significant correlation exists between architecture changes and CI build outcome. In total, we investigated $48$ different correlations using $32$ configurations. Based on our results, we are unable to reject the null hypothesis. Based on our findings, it seems that architectural change does not correlate, and therefore does not impact, with build outcome.

To summarize, the main contributions of this paper are:
\begin{itemize}
\item The first study to investigate the relationship between architectural change and CI build outcome.
\item An open-source extensible framework for 
\begin{itemize}
	\item source code and log mining~\sn{is that part of the framework ?} \jk{yes, the framework is directly connected to the TravisTorrent dataset (interchangable), so I have written a module for log mining. So yes, it's integrated, but more as a side module}
	\item architecture reconstruction
	\item architecture metric calculation
\end{itemize}
\item A large-scale empirical study of almost $50,000$ commits that shows that there is no significant correlation between architectural changes and CI build outcome.
\end{itemize}


\section{Background and Related Work}

In this section, we first give some background about the terminology we use in this paper and then discuss two categories of related work. The first is work done to investigate continuous integration practices and reasons for build failure, and the second is work related to software architecture.

TravisCI is a continuous integration (CI) tool that is tightly integrated into GitHub. CI is the process of automatically compiling, building, and testing a software system, whenever new code is pushed to the single repository. This helps developers find bugs earlier and increase the development spee\footnote{\url{https://www.martinfowler.com/articles/continuousIntegration.html}}. 
The TravisTorrent dataset \cite{TravisTorrent} consists of over 3.5 million TravisCI build samples from over 1,300 projects, written in Java, JavaScript, and Ruby.
TravisTorrent saves over fifty features for every build, including the project name, the commit ID of the build, and the build outcome.

\textit{Software architecture} is defined as the structural or hierarchical view of the system's components and the description of the relationships \cite{arcDef}. It is usually represented as a graph of the high-level components of a software system, called \textit{modules}, which are connected through links, called \textit{dependencies}. While there is no uniform standard for describing a system's architecture, it is typically modeled using Architecture Description Languages (ADLs), such as for example the component diagram of the Unified Modeling Language (UML)~\cite{UML-Arch}. There is a large number of different ADLs, such as Aesop, ATL, or QVT \cite{ADLs1, ADLs2}.
Since there is no single way for describing an architecture, there is also no universally accepted technique for reverse engineering the architecture of a system. The most basic way is using the package structure in the code. This assumes that the developers have thought about the architecture and translated it into packages \cite{arcPkg}. In that case, high level packages correspond to modules, and module A is dependent on module B if an element of package A is using an element that is contained in package B. 
Other approaches for reconstructing architecture are ACDC or ARC, which are introduced later in this section.

\subsection{Continuous Integration}

Continuous Integration is becoming an established technique in software development. Accordingly, there are various different research lines on the topic \cite{ci1, ci2, ci3, ci4}. Given the focus of this paper, we mainly discuss work related to CI build failure prediction.

Islam and Zibran \cite{FailsCorr} have studied the relationship between build outcome, and project and build metrics. 
They found that the size of the project, the size of the test code, and the development branch (i.e. the branch from which the build is initiated) all do not affect the build outcome.
On the other hand, the build tool used (e.g. Maven or Ant) as well as the number of changed lines and files do have an impact on the build outcome.
The fact that previous research found that the number of changed lines and files affects the build result suggests that it makes sense to also investigate the relationship between architectural changes and build outcome, which is the focus of our paper.

In 2006, a study by Hassan and Zhang~\cite{Pred-Tree} used decision trees to investigate the relation between build failures and the meta data of developers and the project.
They achieved an specificity rate\footnote{Specificity rate is the ratio between true negative and all negatives} of 69\%.
In follow-up work by Ni and Li as part of the MSR '17 challenge, this prediction accuracy was further improved, on a per project level, by using cascading tree classifiers as well as additional information about the outcome of the previous build~\cite{Pred-Cascade}. 

Vassallo et al. \cite{CIFailTypes} have combined open and closed source projects to study the reason for build failures. Using the different maven build stages, they documented how many builds failed in which stage. They found several notable differences between open source projects and the systems of the analyzed company. For example, they found that open-source projects fail more often during the testing phase, while closed source projects struggle more during static analysis, deployment, and release phases. They also found that compilation and pre-compilation errors are responsible for around $10\%$ of build failures.
Rausch et al. \cite{FailsinCIFlow} also studied reasons for failing builds. They categorized 14 types of errors which commonly influence the build failure based on 14 open source projects. They found that previous failed builds are highly correlated to build failures, which suggests that failed builds often often appear in groups. In almost all of the studied projects, compilation and dependency resolution were issues for failing builds. Other confirmed metrics that are important for studying build failure were change complexity and failing tests.

\subsection{Architecture}
\label{sec:relwork-arch}

\sn{I quickly skimmed this section but will come back it again. I feel it can be better organized. For example, can you first discuss all extraction tools and methods and then discuss how these tools/methods were used to compare intended and implemented architectues and evolution etc? You can use subsubsections for structuring for example. I feel that right  now it is not that coherent. Can you also include a citation to Gail Murphy's reflexion models http://ieeexplore.ieee.org/abstract/document/917525/ .. while it is not specific to architecture.. I think the reflexion models help find violations of architecture or somethign like that} 
\jk{added another introductory sentence, regrouped papers into conformance and evolution, and what we use}

Software architecture is a well-researched topic in software engineering.
There are generally two directions of research in the area.
The first is related to the reconstruction of the implemented architecture from the code, while the second is related to comparing this reconstructed architecture against (a) the intended or documented architecture and (b) different versions of the system (i.e., architecture evolution).
Since most research about comparing architectures also needs to study reconstruction, we will focus this section on architecture conformance checking and architecture evolution.

\subsubsection{Conformance Checking}

The ARAMIS workbench \cite{Aramis} takes the step from static reconstruction of the architecture based on method calls and inheritance to extracting the data flow during runtime to check it against the predefined structure. 
For model-driven and generative software development, the task of extracting architecture is more complex than in traditional projects, because it passes through different layers of abstraction and possibly multiple DSLs. 
Thus, the way of checking consistency must account for different layers and is abstracted with architecture description languages (ADL) \cite{ArcCons,Arc-MDSE}. 
Also using ADLs, Haitzer et al. \cite{Arc-Decision} study architectural drift and erosion. In their system, the architect can simulate multiple implementation scenarios. With the help of the ADL, the program then calculates the consequences of this change, and the architect can make an educated decision which change has the best impact on the system and its architecture. 

Already in 2001 the Gail Murphy's reflexion models \cite{MurphyRefl} considered the gap between designed and actual architecture of a software system as a problem. For that, they developed a technique to map the reverse engineered model with the designed or expected one to visualize and measure the drift between them. These reflexion models can help find violations in the architecture and are a basis for nowadays architecture conformance checking tools. 

Caracciolo et al. \cite{ArcConf} found that the automated checking of the implemented architecture with the intended one, leads to fewer architecture violations. They extracted the architecture using classic static analysis and a dependency graph. Their system checks then for violations of the MVC pattern and reports it directly to the developer. Over the course of time, fewer violations were found compared to a control group, which shows that the developers learned how to avoid architecture violations.

HUSAACT \cite{Husacct1,Husacct2} is a highly customizable architecture conformance framework for Java and C\#. Dependencies between architecture modules can be defined in multiple ways and different kinds of dependencies are possible. They argue, that a ``call'' dependency cannot be compared directly to a ``inherits'' dependency and, hence, must be treated differently. 
The framework extracts several types of dependencies between source code elements (e.g. classes, interfaces, packages). HUSAACT needs to have a module specification, i.e. which root packages or classes are part of which architecture module. If given this, it maps the defined architecture vs the real implementation and reports violations.
If no such specification is available, it is not possible to compare two versions of software on a higher level than the implementation graph. 
Hillemacher \cite{MScSteffen} has compared four different tools for architecture extraction and conformance checking and elected HUSACCT to be the best extraction tool, which is what we need here.

\subsubsection{Architecture Evolution}

Nakamura and Basili \cite{StructDist} introduced the measuring of architectural distance using kernels. For that, both architectures need to be represented as a graph structure (here OO class structure) and are then compared for similar substructures. This solves the problems with renaming and is applicable to every graph. 
Because it works on the complete class graph and not an abstracted version, i.e. the architecture, this technique is not viable for consistency checking, as the designed architecture is not represented down to the class level. 
Since the graph is compared to kernels, it is necessary to define a distance measurement for this. A similar approach is used by Garcia \cite{arcade-thesis} with the cluster coverage metric.

Tonu et al. \cite{Swag} have researched architecture stability using the SWAG kit, primarily in C and C++ projects, but also introduced support for Java.\footnote{\url{http://www.swag.uwaterloo.ca/javex/}} Using four different types of metrics, growth, change, cohesion and coupling, they try to analyze when the architecture of a software system stabilizes and then predict stability for future versions. It does so by extracting facts from the source code (or in case of Java byte code) and reconstructing the architecture. A fact can be anything, from LoC to function calls and classes. 
In both studied projects, the architecture stabilizes relatively quickly and has only small changes afterwards. Unfortunately, we cannot use this tool in our research, because we want to study the source files and not the byte code of Java projects. But we do analyze change and coupling metrics.

In his PhD thesis \cite{arcade-thesis}, Garcia developed the ARCADE framework. It is used to detect architectural decay, also called thrift and erosion, as well as architecture smells. During this, various recovery techniques were compared \cite{arcRec-comparison}, whereat ACDC \cite{ACDC} and ARC were the most successful. 
ACDC clusters modules based on patterns. Patterns are, for example, if two elements are in the same package, are used together, or if they depend on the same resources. For Java, ACDC recovers the architecture from the class files. ARC is introduced by Garcia himself, clusters entities based on semantic similarity using a statistical language model based on comments and identifiers in the source code.
The ARCADE framework uses these different recovery techniques to compute similarity metrics, namely a2a (architecture to architecture) and cvg (cluster coverage) \cite{Arcade}. 
The first one defines similarity based on the minimum number of steps to get from one architecture graph to the other. For example, removing one dependency or adding one node is each one step. While a2a looks at the architecture from a top-level perspective, cvg looks into the modules, or clusters, and computes if they are still the same module. For example, if all classes in a module are changed but the module name itself is untouched, it cannot still be considered to be the same module. Hence, it compares the entities inside the modules and only considers two modules equal, if at least $x \%$ of the entities are equal.

\subsubsection{Utilization in this Study}

One reason for failing builds are syntax errors. Because builds with syntax errors are uncompilable, we do not want to use recovery techniques which rely on the compiled class files. Thus, we do not use the SWAG Kit or ARCADE directly.
However, the metrics and the recovery technique are promising in our context. Hence, we combine the strengths of HUSAACT in extracting the structure from the source files with the recovery technique ACDC and the metrics from ARCADE. 
In previous studies, other researchers \cite{MScSteffen, arcRec-comparison} have given us confidence that the choices HUSACCT and ACDC are indeed some of the best tools, which we can use.
The cvg metric is comparable to kernel based similarity whereas a2a considers change. Coupling in the architecture will be addressed with Martin's metrics. We reuse the modules from HUSACCT and ARCADE directly, while we need to implement the Martin's metrics ourself. 

\section{Architecture Metrics Used} \label{sec:Metrics}

As part of our work, we need to extract metrics that describe the architectural change that happened in a given version of the code. 
In Section~\ref{sec:relwork-arch}, we provided a general description of several approaches that exist in the literature.
In this section, we provide a detailed description of the eight metrics we use in our work, and the reasoning behind the choice.
A summary of these metrics is shown in Table~\ref{tableMetric}.


\paragraph{Martin's metrics} 
The first set of metrics we use have been developed by Robert Martin~\cite{martinsMetrics}.
We refer to the set of metrics he introduced as \textit{Martin's metrics}.
Specifically, we use three of the five metrics: \textit{afferent} coupling (Aff), \textit{efferent} coupling (Eff), and instability (I).
These metrics describe the independence of modules. 
\textit{Coupling} describes how many incoming (i.e., afferent) or outgoing (i.e., efferent) transitions a module has. 
For example, if class \texttt{Car} of module \texttt{Vehicles} implements the interface \texttt{Engine} of the module \texttt{Parts}, then \texttt{Vehicles} has an outgoing transition to \texttt{Parts}, and \texttt{Parts} has the corresponding incoming transition. 
In other words, \texttt{Vehicles} depends on \texttt{Parts}.
\textit{Instability}, shown in Equation~\ref{eq:inst}, is then defined as the ratio between efferent coupling and the complete coupling.
The equation shows if a module is highly dependent on other modules ($I$ close to 1) or if many modules depend on it ($I$ close to 0).

\begin{equation} \label{eq:inst}
I = \textit{Eff} / (\textit{Eff} + \textit{Aff})
\end{equation}

In the context of architectural change, the instability metric can be used as follows. If the instability of a given module changes, then this module must have had an increase or decrease in its number of dependencies. 
Obviously, a corner case is if the module had the exact same number of changes in its afferent and efferent coupling, leading to a constant instability value despite the changes.

\paragraph{\textit{a2a} and \textit{cvg}} More advanced change metrics are the Architecture to Architecture (a2a) and Cluster Average (cvg) metrics. 
\textit{a2a} measures the minimum amount of steps to get from the first to the second architecture divided by the sum of the steps needed to build both architectures themselves (Equation \ref{eq:a2a}). A step is either the adding, removing, or moving of an entity inside a module or the creation or deletion of a module. 

\begin{gather} \label{eq:a2a}
a2a(A_i, A_j) = 1 - \frac{mto(A_i, A_j)}{aco(A_i) + aco(A_j)}
\intertext{where:}
\begin{tabular}{>{$}r<{$}@{\ =\ }l}
mto(A_i, A_j) & steps from $A_i$ to $A_j$ \\
aco(A_i) & steps to create $A_i$
\end{tabular}\nonumber
\end{gather}


\textit{cvg} is a metric that measures how a given module is changing. For that, the \textit{Cluster to Cluster} (c2c) metric is calculated, which defines how many elements of the cluster overlap divided by the number of elements in the bigger cluster (Equation \ref{eq:c2c}). In the context of software architecture, a cluster is synonymous to a module.
As an example, if a module has 20 elements, and from one version to another, two elements get removed and three are added, then $c2c = \frac{18}{21}$. 
If this ratio is above a predefined threshold $th_{cvg}$, the clusters are considered similar, according to the similarity metric $simC$ in Equation \ref{eq:simC}. The cvg metric then calculates the ratio between equal modules and the amount of modules in the architecture (Equation \ref{eq:cvg}). In ARCADE \cite{Arcade}, the cvg is calculated with a threshold of $75\%$ from the earlier version to the latter one ($\text{cvg}_\text{src}$) and with $66\%$ the other way around ($\text{cvg}_\text{tar}$). These are the standard settings in ARCADE, but it is not explained why these numbers are chosen. One explanation might be that adding functionality to a module does not change its behavior as much as removing elements. Therefore, the higher threshold for $\text{cvg}_\text{src}$ is accounting for this.

\begin{equation} \label{eq:c2c}
c2c(c_i, c_j) = \frac{|\text{entities}(c_i) \cap \text{entities}(c_j)|}{\max(|\text{entities}(c_i)|, |\text{entities}(c_j)|)}
\end{equation}

\begin{equation} \label{eq:simC}
\begin{split}
simC(A_i, A_j) = \{c_i | c_i \in A_i, \exists c_j \in A_j) \\ 
(c2c(c_i, c_j) > th_{cvg})\}
\end{split}
\end{equation}

\begin{equation} \label{eq:cvg}
cvg(A_i, A_j) = \frac{|simC(A_i, A_j)|}{|allC(A_i)|}
\end{equation}

\paragraph{Used Metrics} 
The problem of using a2a and cvg alone is that they do not explicitly take the dependencies between modules into account. 
They are only considered during the reconstruction of the architecture itself using the pattern-based ACDC from the ARCADE framework.
To get a view of the change of the dependencies explicitly, we additionally use the widely accepted Martin's metrics \cite{Val-MartinsMet}, which especially account for the change in between the modules. 

\section{Methodology for Architectural Comparison}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=7in]{assets/combined.pdf}
	\caption{Process from Dataset to Change Metrics. Files are marked in gray. \sn{it is but still everything is the same type of boxes.. you should have different boxes for data vs. processes/steps. For example, Calculator is a step/process while class diagram is an output. I typically like to have boxes for processes and then icons representing the data with a label under them for input/output}~\jk{I followed the UML standards, and the boxes for Artifacts is a little bit different with the little edge on the top right. }}
	\label{architecture}
\end{figure*}

\input{assets/projects}

To establish the relationship between architectural changes and CI build outcome, we need to determine the architectural changes that occurred between two versions of the system.
In this section, we describe the methodology we use to calculate these architectural changes.
Figure \ref{architecture} shows an overview of the methodology we use.
We first read the commit ID of the build out of the TravisTorrent dataset and then download the source code at this commit. Inside the architecture reconstruction step, we first extract the class diagram and then reconstruct the architecture based on the classes. 
The calculator calculates the architectural metrics based on this extracted architecture, and the comparator then compares these metrics against metrics calculated from another commit to compute the architectural change metrics. 
The complete source code of the framework, as well as our results which are explained later, can be found online\footnote{\url{https://github.com/msr18-arch/msr18-arch} - Note that this is an anonymized GitHub repo to comply with the double blind policy.}
%TODO Change back for release
 
\paragraph{Step 1} extracts the unique commit ID of each CI build from the TravisTorrent database.

\paragraph{Step 2} uses the commit ID extracted in step 1 to download a snapshot of the project from GitHub.


\paragraph{Step 3} extracts the architecture information needed to compare two commits in the next step. 
Specifically, Step 3 is split into two parts, (a) the extraction of the class structure and (b) the reconstruction of the architecture. 

In Step 3a, we use the extractor of the HUSACCT framework \cite{Husacct1} to extract relation triples in the form of relationship type, source, target (see listing \ref{lst1:rsf}), and save this information in the Rigi Standard Format (rsf) \cite{RSF}, because the ARCADE tools rely on this format. 
%This relational system is easy convertible into a graph structure or a database and stores the modules implicitly in the dependencies. 
%~\sn{I don't understand the next part about the downside. please elaborate} The downside of this format is that the modules cannot be extracted separately but only implicitly while analyzing the dependencies. That is compareable to the absence of a \texttt{getVertices()} method, and you need to traverse all dependencies to get a full list of the nodes. \jk{does this help?}
%Fortunately, this is irrelevant for our work. 
For simplicity, the dependency types are restricted to ``subPkg'', ``contains'' and ``references'', where ``subPkg'' means that a package is a sub package of another one, ``contains'' means that that a package contains a Java type, and ``references'' is any dependency between two Java types.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.45in]{assets/implementedArc.pdf}
	\caption{Implemented tool chain}
	\label{implToolchain}
\end{figure}

\begin{lstlisting}[
frame = single, 
belowskip=-0.8 \baselineskip,
float,
caption=Rigi Standard Format example,
label=lst1:rsf,
columns=flexible
]
subPkg   checkstyle checkstyle.grammars
subPkg   checkstyle checkstyle.gui
contains checkstyle checkstyle.Main
references checkstyle.Factory checkstyle.Main
\end{lstlisting}

\input{assets/metrics}

In Step 3b, we reconstruct the architecture of the current version of the system based on the information extracted in Step 3a. We use two different architecture reconstruction techniques to ensure that any findings we have are not as a result of a problem or bias in one technique.

The first is ACDC, because it has one of the best reconstruction accuracy rates when compared to other commonly used techniques \cite{arcRec-comparison}. 
While an implementation of ACDC is provided in the ARCADE framework, this implementation works directly on the Java byte code.  Given that a common reason for failing builds is compilation errors~\cite{CIFailTypes}, it is thus better if we can instead directly use the source code for extracting the architecture. This is why we have Step 3a where we extract the class structure using the source-based HUSACCT extractor, and then feed that information into ACDC after deactivating its own first-level extraction.

The second reconstruction technique we use is package reconstruction, which is often used to compare the implemented architecture to the documented one \cite{arcRec-comparison}. 
We implemented this technique ourselves based on the HUSACCT extraction.
This technique looks for the root packages in the system. If there are only a few root packages, it is assumed that the implemented architecture lies a level deeper in the packages. For example, if there is only the ``org.sonar'' as root, this is just the base container and not an architectural element. Therefore, all direct children of this package are considered as modules until there are at least an acceptable number of modules. In this work, we consider 10 as the minimum amount of modules needed. This number was chosen with manual sampling over different projects, such that the achieved results made sense to us after looking at the system and trying to understand its intended architecture. We follow Song et al.'s approach of considering all ``references'' edges as dependencies~\cite{ArcAsGraph}. 

\paragraph{Step 4} calculates the architectural changes between two given commits.
As explained in Section \ref{sec:Metrics}, we use the ARCADE metrics a2a and cvg as well as Martin's metrics. We calculate the ARCADE metrics based on the architecture extracted by ACDC. Cvg is calculated twice, one based on the modules of the first version (called \textit{source}) and one based on the second version (called \textit{target}).
The coupling metrics are calculated on the package architecture to have some ground truth metrics for comparison. We calculate five package metrics: first the number of modules and dependencies in the graph, then the node degree and, at last, the two different instabilities (Martin's metrics); thus, we have 8 metrics in total summarized in Table \ref{tableMetric}.
For every module, the absolute number of incoming and outgoing edges are calculated, the degree of the node (sum over ingoing and outgoing edges), as well as the absolute and relative instability. The absolute instability is based on the absolute number of connections between the modules, i.e. if Module A has only three ingoing connections from Module B, then the afferent coupling is considered three. For the relative instability, the coupling is booleanized, i.e. is $0$ if there is no connection or $1$ if there is one, so it is not measured how tightly connected two modules are, just if they are connected at all.
Since we need to calculate the change for the complete architecture, and not for each module, we calculate the mean instabilities and node degree as well as the absolute number of modules (vertexes) and dependencies (edges).

We then perform a pairwise comparison of each metric. To compare proportional metrics, like instability, the difference between the values is taken, e. g. if architecture A has an average instability of $30\%$, and architecture B has an average instability of $25\%$, then they are $5\%$ different to each other (Equation \ref{eq:relDiff}). For metrics with absolute numbers, like node degree, we calculate the similarity as the proportion of the two values, e.g. A has an average node degree of $8$ and B has $10$, then they are $80\%$ similar, i.e. $20\%$ change (Equation \ref{eq:absDiff}).

\begin{equation} \label{eq:relDiff}
c_{rel}(m_1, m_2) =  \max\{m1, m2\} - \min\{m1, m2\}
\end{equation} 

\begin{equation} \label{eq:absDiff}
	c_{abs}(m_1, m_2) = 1 - \frac{\min\{m1, m2\}}{\max\{m1, m2\}}
\end{equation} 


We save the corresponding comparison results of each pair of architectures, corresponding to two consecutive commits, in a JSON file. Given the architecture of our framework, our methodology can easily be expanded with more architecture reconstructors or metric calculators. 

To add another module to the framework, one needs only to implement a new subclass for the changing module, store its result in the Rigi Standard Format and it can be integrated with all existing tools. 
This is how we managed to combine HUSACCT and ACDC and integrate them with the ARCADE metrics. Similarly, we have implemented the package reconstruction and the metrics that are based on the package architecture, so that both ways of computing change can be run be run individually or simultaneously (Fig. \ref{implToolchain}).

\section{Methodology for CI Build Outcome Extraction}
\label{sec:ci-build-outcome}

The extracted architecture change metrics are compared and correlated to software faults. In this study, we use the CI build outcome as a metric for faults. 
As Vassallo et al. \cite{CIFailTypes} described, there are different types of build failures, classified based on the Maven goals. 
%I do not understand the next part at all so I tried to simplify
%Given that we already introduced compilation errors as one possible source for failing builds, this is a sound approach. 
Because it is very costly to run every maven goal separately for every build we analyze and because some builds have dependencies that need to be manually installed, it is not feasible to re-build the system ourselves for every trigger commit.
Instead, we use the following simplified strategy to identify the reason for the failed build.
%Furthermore, because some builds are already older and have dependencies which are not anymore available or need to be installed manually, the approach to rerun everything may even be impossible. 
%Still, we base our heuristic on the Maven goals.
We first differentiate between failed and passed builds by looking at their status in the TravisTorrent dataset. For failed builds, we first identify lines that start with the ``\texttt{[Error]}'' keyword, which is used by the Maven log to indicate defects. Afterwards, we look for the keywords ``Compilation failure'', ``dependencies'', or ``test failures'' to further categorize the failures~\sn{Are these exact words used for the search?}\jk{almost. I think I searched for "There are test failures" which is the hard coded phrase in maven}. Accordingly, we end up with five possible build results: (0) no error, (1) error during dependency resolution, (2) compilation failure, (3) test failure, or (4) unknown error. The fourth category is for errors that we could not identify using our heuristic.

\section{Evaluation Setup}

We now explain the setup we use for our evaluation.

\paragraph{Data Set.}
To investigate our hypotheses, we analyze projects from the TravisTorrent dataset, which use Java and Maven. 
We have restricted ourself to Maven, because the build tool impacts the outcome of the build but not the architecture itself \cite{FailsCorr}. Although this introduces bias, it ensures consistent results in return. HUSACCT needs the path to the source files, so we can exclude test files and resources. Maven has a convention over configuration paradigm. In this way, we can be sure where everything is stored in the file structure. This eases the implementation of the class structure extraction.

The TavisTorrent dataset contains $241$ projects that use Java as their main programming language and are built with Maven. 
From these $241$ projects, we exclude ones that have have less than two commits on the master branch, because we need at least two commits to be able to compare architectures. 
This left us with \todo{$X$} projects.
We further filter this list to exclude projects where we could not retrieve their builds or logs from GitHub and TravisCI~\sn{why would this ever happen? aren't these stored in the travistorrent data set?} \jk{no, only the IDs which link to the server, but some are just not present anymore}, or where our toolchain was unable to reconstruct the architecture~\sn{again why would this happen?} \jk{bugs, download errors .. various reasons, some are problems in the code, others problems with the used tools.. and some I just don't know. That's why I just gave the beginning and the end number, because we lost here the most projects. But I don't think it would change the results.}

Some commits get built more than once. This happens if the configuration of the CI server changes, a developer starts another build without changing code or if builds get started independently of the pushed code.
The only thing that can happen that affects the build outcome is a changing configuration. We haven chosen only to consider the first build that uses this commit, because it is impossible that the architecture changes between these builds.
Thus, we can ensure that changing configuration does not bias the results. Instead, we compare the changing source code based on the older configuration, i.e. the configuration which is equal to the one of the older build. 
This left us with $159$ projects to analyze, containing a total of
$49,531$ commits. A list of the ten biggest analyzed projects, in terms of total number of commits~\sn{what is an unrestricted commit?}\jk{bad wording, total number of commits and not number of commits after our restrictions took place}, can be found in table \ref{tableProjects}.

The pass rate of the studied projects is $84.9\%$. Around $10\%$ of the builds failed because of errors before or during the compilation of the software. One third of the errors could be mapped to test errors. We did not further separate the other types of failures, but sampling shows that some of the reasons for the other category include configuration errors or incorrect maven plugins.
The active time of the projects in the data set varies.
For the ten biggest projects (in terms of number of commits), it is between one and four years of development time, some of which start at a well-established state (the TravisTorrent dataset starts, for example, at build number 500 for SonarQube), while others start at build 1 (e.g. Checkstyle)~\sn{ is the part about where they start general to all projects or to the top 10?} \jk{does it matter? If it's true for 10, it is also true for all.}.
The projects also belong to a range of different domains.
In the ten biggest projects, there are two code analyzers, three APIs~\sn{libraries? and what kind of libraries? libraries have domains too} \jk{c.f. table 1. I thought it sounds better to say three APIs then, one x API, one y API and one z API. And I used the self description of the projects for this, if they classified themself as API, then I said it's an API}, a web client and an IDE, some for databases, linear algebra or ontologies. Table \ref{tableProjects} gives a detailed list.


\paragraph{Calculated Correlations.}

The calculated architectural metrics are tested for correlation with the build result. This is done in multiple ways. First, the metric is tested against the direct build outcome, then the previous or following $b$ outcomes for $b \in \{2, 3, 5, 10\}$. This means that we look for correlations between a change in build $x$ and the build result in builds $\{x, x \pm 1, \hdots, x \pm b\}$. From this, we can see if architectural change correlates to build failures in recent history or near future (i.e., the relationships mentioned in our hypothesis). 
~\sn{Before I try to rewrite this, I need to  understand why you don't consider the individual types of build failures if you differentiate between them? So if you do record why the build failed, why don't you see if there is a correlation with failure in general and then with the individual types of failure?}\jk{I consider the type of failure in the direct correlations with the corresponding build. If I do it with multiple builds, then I have to correlate AC: x \% with the next n Builds: [0, 1, 2, 2, 0], which would be 0 succ builds, 1 error type A, 2 error type B, 2 error type C etc. And I didnt' had a idea how to measure this type of correlation. Then i simplified it to "in the next 5 builds, there are 2 failures".} Because there are different reasons for a CI build failure (see Section~\ref{sec:ci-build-outcome}), for testing in the near past and future, the fail types get categorized into non-failure and failure. In this way, we get a single integer of how many builds failed, instead of having multiple variables. 
Then, for every architectural metric, the change gets classified from its continuous value into the two categories 'change' and 'no-change'. For that, we consider a threshold $t \in \{0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5\}$, where every change above this threshold is considered a change, and everything below as no change. 
We chose this range of thresholds, because most of the architectural changes in our dataset were small (Figure \ref{fig:histograms}). 
We take different thresholds at all to figure out if small changes affect the measured correlation. 
For every calculated comparison that includes no Boolean variables, the correlation test is run with Pearson's test, while others are run with Spearman's test. ~\sn{this previous paragraph generally needs another revision} \jk{better?}

\begin{figure*}[!t]
	\centering
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/NumNodes.pdf}
		\caption{\#V}
		\label{numNodesHist}
	\end{subfigure}%
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/NumEdges.pdf}
		\caption{\#E}
		\label{numEdgesHist}
	\end{subfigure}%
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/RelInst.pdf}
		\caption{rel. Inst}
		\label{relInstHist}
	\end{subfigure}
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/NodeDegree.pdf}
		\caption{deg(V)}
		\label{nodDegHist}
	\end{subfigure}%
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/a2a.pdf}
		\caption{a2a}
		\label{a2aHist}
	\end{subfigure}%
	\begin{subfigure}{2.3in}
		\centering
		\includegraphics[width=2.3in]{assets/cvgSource.pdf}
		\caption{cvg src}
		\label{cvgHist}
	\end{subfigure}
	\caption{Histograms metric over all projects}
	\label{fig:histograms}
	
\end{figure*}

\section{Results}

This section presents the findings with the proposed methodology. First, the projects under study are introduced, then calculated metrics are evaluated and, eventually, the correlation between the metrics and the build outcomes is shown.

\subsection{Evaluation of Architecture Metrics}

\input{assets/metricCorr}

We have extracted 8 architecture change metrics. 
Table~\ref{tableMetric} shows the number of percentage of builds that show changes using the corresponding metric, i.e. the number of builds with a value greater than 0.
We can see that most builds show no change.
This is to be expected, since most changes in the system should not be architectural changes. 
Only a2a has more changing builds than non-changing builds. 
However, an investigation of these changes in Figure~\ref{a2aHist} show that most of them have a really small a2a value. 
Most other plots in Figure \ref{fig:histograms} show that for the other metrics, most changes are quite small as well, with only few big architectural changes. 
Only the number of nodes shown in Figure \ref{numNodesHist} has more widespread changes.
This could be due to the nature of the reconstruction process. ~\sn{I don't understand the next part. Please rewrite} Since we enforce a minimum number of ten nodes per architecture in the package-architecture reconstruction, we cannot have nine only nine modules. If the architecture is now changing from ten to nine, another layer of module separation is exectured. This will result in a change that is way bigger, because most packages will have more than one sub package. So the change will be most likely from ten to something above 18. The effects of this red line are not as bad as they sound, because the node degree and number of edges metrics are in the expected norm, so that it is indicated that the few outliers do not affect the results badly.

Before using the calculated metrics in further analyses, we need to ensure that our chosen metrics make sense in practice and do not all measure the exact same aspect of architectural evolution. Thus, we calculate the Pearson correlation between all the metrics we use, shown in Table \ref{tableMetricCorr}. All p values for these correlations were equal or close to 0; the highest p value we got from all the cross-metric correlations is $3.7 \cdot 10^{-102}$. 
We consider a correlation above $60\%$ as highly correlated, and everything below $30\%$ as low correlation.
The results show that the package metrics (i.e. every metric that is calculated on the package architecture) are highly correlated, all with correlation values above $0.6$ up to $0.92$. a2a does not have any strong correlation with any of the other metrics (all values between $0.10-0.17$), while cvg shows low to medium correlation with the package metrics. 
These correlations suggest that all metrics are truly an indicator for architectural change if we assume that at least one of them is, since the p-values do not leave any doubt that the metrics are dependent.
But, as expected, the correlation is not over the roof top, because the metrics look at different angles of architectural change. But since they all look at change, they have to be correlated somehow. 
a2a and cvg highlight the change at the module level, whereas the Martin's metrics focus on the relationship between the modules. This strengthens our confidence in the chosen tool chain. ~\sn{you somehow say they are dependent but then say they look at disjunct types of changes.. can you revise the text to make the poitn clearer} \jk{better?}


\subsection{Relationship Between Architectural Changes and Build Outcome}

Around $16\%$ of all builds that had architectural changes failed ($P(f|c)$). Of course, depending on the change threshold taken, this number ranges. But all metrics showed at least at some point these $16\%$. The complete range of the values is between $8\%$ and $18\%$. 
If we check for the probability of change under the condition that the build failed ($P(c|f)$), we get a range from $0\%$ up to $60\%$, based on the metric and the change threshold. For the package metrics, the probability was almost always low, but the ARCADE metrics have more interesting results. First, the probability drastically decreases as the change threshold increases. If we look at the probability of architectural change according to the a2a metric and we consider every change (i.e. change threshold at $0.0$, then we end up with a $61\%$ probability. Of course, this is natural since we do not change the number of failing builds but the number of considered changes when we change the threshold of what we consider a change. 
Because of the constant values around $p(f|c) = 16\%$ and peak values for $p(c|f)$ with $61\%$ for a2a and $35-36\%$ for cvg, we are led to the conclusion that there is the possibility for correlation between the data.

\subsection{Correlation}

\input{assets/allCorr}

When calculating the correlation between architectural change and build outcome, we got the values as mentioned in table \ref{tab_corr}. As you can see some of the metrics show no correlation at all ($c < 0.4\%$ and $p > 0.25$), while the top metrics, a2a and cvg, peak at $4.3\%$ ($p = 6.8 \cdot 10^{-22}$) and $3.0\%$ ($p=1.2 \cdot 10^{-11}$). 
The threshold that controls how many previous or following builds are considered had no large impact. Only when going using a very large window, with the previous $40$ builds, do we start to see a significant rise in the correlation, as shown in Figure \ref{a2aPrevN}). However, a rise from $3\%$ to $6\%$, although it has doubled, is not that interesting. Furthermore, to consider the previous $80$ builds, where the peak occurred, is likely due to chance than to actual correlation~\sn{wouldn't the p-value have told you this?} \jk{why would it? The p-value doesnt know anything about the structure of the data}. In other words, it is improbable that code that was committed up to $80$ builds before the current one, influences the architecture. ~\sn{do you consider each of the 80 builds or literally the build 80 commits before?} \jk{each of the builds, I added "up to" to clarify}
Nevertheless, in Figure \ref{a2aPlot}, we computed the correlation values over the next $80$ builds for different change thresholds, and confirmed our intuition that the correlation is even lower than the one for the next $10$ builds after the initial peak.
The highest correlation found for a2a was the one with a change threshold of $0.0$, i.e. every little change was considered as an architectural change. However, even the strongest correlation we found, in a realistic scenario, was only $4\%$ over the next ten builds~\sn{I don't see the blue line at 4 percent or is it just hidden by the red?} \jk{just hidden, confirm the new table to check}. 

Some of the metrics have low p-values, which indicates that there is a statistically significant relationship between build outcome and those metrics. However, the detected correlation is so small that we cannot conclude this as a meaningful impact. Because taking the smallest changes into consideration increases the correlation coefficient, and the threshold for previous and following builds had no significant change on the correlation coefficient, we conclude that the stronger correlations could be noise. 
The strongest correlation was found with the a2a metric, which works on the modules and does not consider the relationship between them. Therefore, the possible correlation happens not in the interaction between the modules, but the modules themselves. Furthermore, over all correlations, the tendency was towards a positive value, with only $38\%$ negative correlation values. Because we marked no build error with a zero, this shows that change is occurring more often around failing builds, whereas a static architecture shows in the near of non-failing builds. That is, if there truly is a correlation.
~\sn{how did you conclude that?} \jk{oh youre right, thats the other way around .. my mistake. Correlation ranges from -1 to 1 and -1 is a perfect negative correlation, we have a positive one, which says if change is rising, then the build outcome is rising. And a "low build outcome" is 0 which is no error}

\begin{figure}[!t]
	\centering
	\includegraphics[width=3in]{assets/PrevN}
	\caption{a2a vs previous n builds over 0.0 threshold }
	\label{a2aPrevN}
\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width=3in]{assets/a2aCorr}
	\caption{a2a vs next ten and eighty builds over different change thresholds }
	\label{a2aPlot}
\end{figure}


\section{Discussion and Threats to Validity}

After analyzing $159$ Java projects, we found no significant correlation between architectural change and CI build outcome. Apart from there actually being no correlation, there could be various factors influencing this result. 

First of all, only Java projects that use Maven were evaluated. Hence, we cannot generalize our results globally to all Java projects or to other programming languages. However, we cannot think of a reason why the choice of Maven could positively or negatively impact our results. Given that the toolchain and framework we use are publicly available and designed in a way to facilitate their extension, future work can investigate projects suing other build systems.

Despite having projects which start at build one in our data set, the number of failing builds in the data set was relatively small. 
It is possible that the missing connection between architecture and build results lies in the possibility that the big projects which have a stronger impact on the results have a well thought-through architecture and do not change it in a problematic way.
This can be backed up by the fact that most of the architectural changes we found are below a change threshold of $10\%$. It would be interesting to see if projects with a more unstable architecture show more failures. We have not found interesting cross-project differences.

It is also possible that the problems which are produced by architectural change are not transferred into build results but to a different level, e.g. (1) code coverage or (2) number failed tests~\sn{but then I'm confused as to how you got the test results as you mentioned above? Also, doesn't the mvn log mention the number of passed/failed tests?} \jk{yes, but some logs are missing. The TravisTorrent dataset has 20\% missing values for number of failed tests, as said in the next sentence}. Unfortunately, the TravisTorrent dataset does not give the opportunity to check for those, because they are (1) not reported or (b) have missing values in around $20\%$ of the builds. Software architecture is important for the successful development of software systems \cite{ADLs1}, and our detection of changes show that the architecture does change over the lifetime of a project. However, it remains unknown how architecture change affects the progress of the continuous integration process.

As Garcia et al. \cite{arcRec-comparison} show even the best recovery techniques have problems detecting the true architecture. Therefore, it is possible that HUSACCT and ARCADE are not the right tools for the research question. We addressed this problem by considering multiple recovery techniques and relying on studies that evaluated ARCADE as being one of the most accurate recovery tools. The produced metrics highly correlate, which gives us confidence that the change detection is working as intended. Still, it is possible that our chosen metrics may not be the best set. For example, because of their strong correlation, they would not be a suitable feature set to use in a machine learning technique that predicts the build outcome. For such a task, a broader set of metrics may be necessary. 

In the future, more metrics and extractors can be added to our framework.  
Then, it is necessary to extend the dataset with more software systems which actively change their architecture. In the optimal case, there are even some software system which evolved their architecture completely. 
Because this work introduces an easy expandable framework, this should be possible with little to no effort.

\section{Conclusion}

We analyzed $49,531$ builds from $159$ different Java Maven projects from various domains to determine the relationship between architectural changes and continuous integration build outcome.
We designed an extensible framework that currently uses one extractor to identify source code facts, two architecture reconstructors, and three architecture evolution metrics based on previous research.
In total, we tested the correlation between $8$ architecture change metrics and build outcome in $6$ different constellations, including the previous and following builds, and multiple change thresholds.
Our results show that there is no significant relationship between architectural changes and build outcome.

The hypothesis that architectural change impacts CI build outcomes at or near the change was rejected. The highest correlation found was around $4\%$ and would imply that architectural changes lead to fewer failures. 
We discussed possible reasons for this negative result.
All our results are published online, and we encourage other researchers to replicate our results or to add additional metrics that we may have missed through our publicly available extensible framework.

\bibliographystyle{ACM-Reference-Format}
\bibliography{literature}

\end{document}
\grid
